{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiG8X2IkH7BNBX8wkrjFlJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/untergeekDE/KInsKI-und-andere-spielereien/blob/main/KInsKIBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-telegram-bot\n",
        "!pip install aleph-alpha-client\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II00AUu8PT9I",
        "outputId": "7a4d6f18-cbad-4782-b338-b4a99827bf18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-telegram-bot in /usr/local/lib/python3.7/dist-packages (13.14)\n",
            "Requirement already satisfied: APScheduler==3.6.3 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (3.6.3)\n",
            "Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (4.2.2)\n",
            "Requirement already satisfied: tornado==6.1 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (6.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (2022.9.24)\n",
            "Requirement already satisfied: pytz>=2018.6 in /usr/local/lib/python3.7/dist-packages (from python-telegram-bot) (2022.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (1.15.0)\n",
            "Requirement already satisfied: tzlocal>=1.2 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (1.5.1)\n",
            "Requirement already satisfied: setuptools>=0.7 in /usr/local/lib/python3.7/dist-packages (from APScheduler==3.6.3->python-telegram-bot) (57.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: aleph-alpha-client in /usr/local/lib/python3.7/dist-packages (2.4.4)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.7/dist-packages (from aleph-alpha-client) (2.28.1)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.7/dist-packages (from aleph-alpha-client) (1.26.12)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->aleph-alpha-client) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->aleph-alpha-client) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.28->aleph-alpha-client) (2022.9.24)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.7/dist-packages (0.23.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from openai) (4.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from openai) (1.21.6)\n",
            "Requirement already satisfied: pandas-stubs>=1.1.0.11 in /usr/local/lib/python3.7/dist-packages (from openai) (1.2.0.62)\n",
            "Requirement already satisfied: openpyxl>=3.0.7 in /usr/local/lib/python3.7/dist-packages (from openai) (3.0.10)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.28.1)\n",
            "Requirement already satisfied: pandas>=1.2.3 in /usr/local/lib/python3.7/dist-packages (from openai) (1.3.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2022.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.26.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "API-Token für den Bot und die Services einlesen (wie üblich als Textdatei im GDrive)"
      ],
      "metadata": {
        "id": "Zr2PPMjx5HP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "keyfile = open('/content/drive/MyDrive/keys/kinski_bot.txt','r')\n",
        "tg_bot_token = keyfile.readline()\n",
        "keyfile.close()\n",
        "\n",
        "keyfile = open('/content/drive/MyDrive/keys/aa_key.txt','r')\n",
        "aa_token = keyfile.readline()\n",
        "keyfile.close()\n",
        "\n",
        "keyfile = open('/content/drive/MyDrive/keys/openai_key.txt','r')\n",
        "openai_token = keyfile.readline()\n",
        "keyfile.close()\n",
        "\n",
        "print(\"AlephAlpha Token (MD5)\", hashlib.md5(aa_token.encode('utf-8')).hexdigest(),\"\\n\")\n",
        "print(\"OpenAI Token (MD5)\", hashlib.md5(openai_token.encode('utf-8')).hexdigest(),\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw-OkYn95CTC",
        "outputId": "d1714562-c520-40d3-ae78-4dd63a985f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "AlephAlpha Token (MD5) d7644b85dbe845bf36c85a166ee72f41 \n",
            "\n",
            "OpenAI Token (MD5) 4dd938fcec2f1d51f9aa58083aa1d3b5 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das Prompt einlesen: (Kurzbeschreibung und Frage-Antwort-Paare; endet auf eine Kinski-Antwort)\n",
        "\n"
      ],
      "metadata": {
        "id": "tvsRXjs97plS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lies die Kinski-Texte ein - ohne Frage\n",
        "def load_kinski_prompt(): \n",
        "  promptfile = open('/content/drive/MyDrive/kinski_prompt.txt','r')\n",
        "  promptlines = \"\".join(promptfile.readlines())\n",
        "  promptfile.close()\n",
        "  return(promptlines)"
      ],
      "metadata": {
        "id": "H2C1DDBn7nPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hier die Antwortfunktion mit Aleph Alpha als Service:"
      ],
      "metadata": {
        "id": "RQr9CHdL6EQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from aleph_alpha_client import AlephAlphaModel, AlephAlphaClient, CompletionRequest, Prompt\n",
        "model = AlephAlphaModel(\n",
        "    AlephAlphaClient(host=\"https://api.aleph-alpha.com\", token=aa_token),\n",
        "    model_name = \"luminous-supreme\"\n",
        ")\n",
        "\n",
        "def aleph_complete(prompt):\n",
        "  request = CompletionRequest (\n",
        "      prompt = Prompt(prompt),\n",
        "      maximum_tokens = 256,\n",
        "      temperature = 1.0,\n",
        "      top_k = 10,\n",
        "      top_p = .27,\n",
        "      use_multiplicative_presence_penalty=True,\n",
        "      presence_penalty=0.6,\n",
        "      frequency_penalty=0.6,\n",
        "      stop_sequences = [\"###\"],\n",
        "\n",
        "    )\n",
        "  result = model.complete(request)\n",
        "  t = result.completions\n",
        "  return(t[0].completion)\n",
        "\n",
        "print(\"AA-Funktion definiert\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wQXUp6s6D4K",
        "outputId": "a09d3a4f-f466-4a5d-db77-4ec7742ad46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AA-Funktion definiert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI-Completion-Funktion"
      ],
      "metadata": {
        "id": "pSoY7JAU5Uiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = openai_token\n",
        "\n",
        "def openai_complete(p):\n",
        "  response = openai.Completion.create(\n",
        "    model=\"text-davinci-002\",\n",
        "    prompt=p,\n",
        "    temperature=1,\n",
        "    max_tokens=256,\n",
        "    top_p=0.27,\n",
        "    best_of=3,\n",
        "    frequency_penalty=0.6,\n",
        "    presence_penalty=0.6,\n",
        "    stop=[\"###\"]\n",
        "  )\n",
        "  return(response.choices[0].text)\n",
        "\n",
        "  #return(response)\n",
        "print(\"OpenAI-Completion definiert\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaVAU_of5VNs",
        "outputId": "b559a8e9-2a70-4668-8146-460cae68091d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI-Completion definiert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Bot"
      ],
      "metadata": {
        "id": "G_4voROR5E0r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzDZQdTBPQfo",
        "outputId": "c6b61a8c-eda1-4372-aa3a-98e6aef56fc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:Update \"{'update_id': 692851969, 'message': {'delete_chat_photo': False, 'new_chat_members': [], 'chat': {'username': 'untergeek', 'last_name': 'Eggers', 'first_name': 'Jan', 'id': 5248931, 'type': 'private'}, 'supergroup_chat_created': False, 'channel_chat_created': False, 'date': 1665172702, 'caption_entities': [], 'entities': [{'offset': 0, 'type': 'bot_command', 'length': 5}], 'text': '/help', 'new_chat_photo': [], 'message_id': 14, 'photo': [], 'group_chat_created': False, 'from': {'username': 'untergeek', 'last_name': 'Eggers', 'language_code': 'de', 'first_name': 'Jan', 'id': 5248931, 'is_bot': False}}}\" caused error \"can only concatenate str (not \"int\") to str\"\n",
            "WARNING:__main__:Update \"{'update_id': 692851970, 'message': {'delete_chat_photo': False, 'new_chat_members': [], 'chat': {'username': 'untergeek', 'last_name': 'Eggers', 'first_name': 'Jan', 'id': 5248931, 'type': 'private'}, 'supergroup_chat_created': False, 'channel_chat_created': False, 'date': 1665172706, 'caption_entities': [], 'entities': [{'offset': 0, 'type': 'bot_command', 'length': 5}], 'text': '/info', 'new_chat_photo': [], 'message_id': 15, 'photo': [], 'group_chat_created': False, 'from': {'username': 'untergeek', 'last_name': 'Eggers', 'language_code': 'de', 'first_name': 'Jan', 'id': 5248931, 'is_bot': False}}}\" caused error \"can only concatenate str (not \"int\") to str\"\n",
            "WARNING:__main__:Update \"{'update_id': 692851971, 'message': {'delete_chat_photo': False, 'new_chat_members': [], 'chat': {'username': 'untergeek', 'last_name': 'Eggers', 'first_name': 'Jan', 'id': 5248931, 'type': 'private'}, 'supergroup_chat_created': False, 'channel_chat_created': False, 'date': 1665172730, 'caption_entities': [], 'entities': [{'offset': 0, 'type': 'bot_command', 'length': 5}], 'text': '/help', 'new_chat_photo': [], 'message_id': 16, 'photo': [], 'group_chat_created': False, 'from': {'username': 'untergeek', 'last_name': 'Eggers', 'language_code': 'de', 'first_name': 'Jan', 'id': 5248931, 'is_bot': False}}}\" caused error \"can only concatenate str (not \"int\") to str\"\n"
          ]
        }
      ],
      "source": [
        "# Globale Variablen\n",
        "bot_version = \"v0.1\"\n",
        "prompt = \"\"\n",
        "n_fragen = 0\n",
        "model = \"aleph\"\n",
        "\n",
        "\"\"\"\n",
        "Simple Bot to reply to Telegram messages.\n",
        "\n",
        "First, a few handler functions are defined. Then, those functions are passed to\n",
        "the Dispatcher and registered at their respective places.\n",
        "Then, the bot is started and runs until we press Ctrl-C on the command line.\n",
        "\n",
        "Usage:\n",
        "Basic Echobot example, repeats messages.\n",
        "Press Ctrl-C on the command line or send a signal to the process to stop the\n",
        "bot.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "\n",
        "from telegram.ext import Updater, CommandHandler, MessageHandler, Filters\n",
        "\n",
        "# Enable logging\n",
        "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "                    level=logging.INFO)\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Define a few command handlers. These usually take the two arguments update and\n",
        "# context. Error handlers also receive the raised TelegramError object in error.\n",
        "def start(update, context):\n",
        "    global prompt\n",
        "    prompt = load_kinski_prompt()\n",
        "    global n_fragen\n",
        "    n_fragen = 0\n",
        "    \"\"\"Send a message when the command /start is issued.\"\"\"\n",
        "    update.message.reply_text('Herr Kinski ist jetzt da. Sie können Ihre Frage stellen.')\n",
        "\n",
        "\n",
        "def help(update, context):\n",
        "    global n_fragen\n",
        "    global model\n",
        "    \"\"\"Send a message when the command /help is issued.\"\"\"\n",
        "    update.message.reply_text(\"### KInsKI Bot \" + bot_version + \" ###\\n\\n\" \\\n",
        "                              + \"Chatte mit Klaus Kinski - in einer KI-Variante: \" \\\n",
        "                              + \"Die KI-Sprachmodelle OpenAI GPT-3 und Aleph Alpha Luminous Extreme \" \\\n",
        "                              + \"antworten, wie Kinski geantwortet hätte. \\n\\n\" \\\n",
        "                              + \"Spätestens nach 10 Fragen ist Schluss. Ask wisely.\\n\\n\" \\\n",
        "                              + \"Spenden für die Modell-Kosten per Paypal an jan@eggers-elektronik.de\\n\\n\" \\\n",
        "                              + \"*Befehle:* \\n\" \\\n",
        "                              + \"/help, /info - diese Hilfe\\n\" \\\n",
        "                              + \"/start - Neues Gespräch starten\\n\" \\\n",
        "                              + \"/openai - Schalte auf Modell GPT3\\n\" \\\n",
        "                              + \"/aleph - Schalte auf Modell Luminous\\n\" \\\n",
        "                              + \"Fragen gestellt: \" + n_fragen + \", Modell: \" + model)\n",
        "\n",
        "\n",
        "def KI_wrapper(frage):\n",
        "    global prompt\n",
        "    global model\n",
        "    p = prompt + \"###\\nFrage: \" + frage + \"\\nKinski:\"\n",
        "    if model == \"aleph\":\n",
        "      antwort = aleph_complete(p)\n",
        "    else:\n",
        "      antwort = openai_complete(p)\n",
        "    return(antwort)\n",
        "\n",
        "def answer(update, context):\n",
        "    \"\"\"Echo the user message.\"\"\"\n",
        "    global prompt\n",
        "    global n_fragen\n",
        "    if n_fragen < 10:\n",
        "      antwort = KI_wrapper(update.message.text)\n",
        "      update.message.reply_text('*' + antwort + '*')\n",
        "      prompt = prompt + \"###\\nFrage: \" + update.message.txt + \"\\nKinski: \" + antwort \n",
        "      n_fragen += 1\n",
        "      if n_fragen > 8: \n",
        "        update.message.reply_text('*Letzte Frage jetzt, sorry!*')\n",
        "    else:\n",
        "      update.message.reply_text('Herr Kinski musste wirklich gehen. \\n\\n' \\\n",
        "                                + 'Komm später wieder oder starte eine neue Session mit /start\\n'\n",
        "                                + 'Wenn dir KInsKI gefällt: Spenden für den Betrieb an die Paypal-' \\\n",
        "                                + 'Adresse jan@eggers-elektronik.de - weil jede Antwort ein paar ' \\\n",
        "                                + 'Cent in Rechenzeit kostet... danke!\\n')\n",
        "\n",
        "\n",
        "def set_aleph(update, context):\n",
        "    \"\"\"Switch to Aleph Alpha model.\"\"\"\n",
        "    model = \"aleph\"\n",
        "    update.message.reply_text('Aktives Modell jetzt: Aleph Alpha Luminous (Supreme)\\n')\n",
        "\n",
        "def set_openai(update, context):\n",
        "    \"\"\"Switch to OpenAI model.\"\"\"\n",
        "    model = \"openai\"\n",
        "    update.message.reply_text('Aktives Modell jetzt: OpenAI GPT-3 (Da Vinci)\\n')\n",
        "\n",
        "def error(update, context):\n",
        "    \"\"\"Log Errors caused by Updates.\"\"\"\n",
        "    logger.warning('Update \"%s\" caused error \"%s\"', update, context.error)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Starte KInsKI.\"\"\"\n",
        "    global prompt\n",
        "    prompt = load_kinski_prompt()\n",
        "    global n_fragen\n",
        "    n_fragen = 0\n",
        "    global model\n",
        "    model = \"aleph\"\n",
        "\n",
        "\n",
        "    \"\"\"Start the bot.\"\"\"\n",
        "    # Create the Updater and pass it your bot's token.\n",
        "    # Make sure to set use_context=True to use the new context based callbacks\n",
        "    # Post version 12 this will no longer be necessary\n",
        "    updater = Updater(tg_bot_token, use_context=True)\n",
        "\n",
        "    # Get the dispatcher to register handlers\n",
        "    dp = updater.dispatcher\n",
        "\n",
        "    # on different commands - answer in Telegram)\n",
        "    dp.add_handler(CommandHandler(\"info\", help))\n",
        "    dp.add_handler(CommandHandler(\"help\", help))\n",
        "    dp.add_handler(CommandHandler(\"openai\", set_openai))\n",
        "    dp.add_handler(CommandHandler(\"gpt3\",set_openai))\n",
        "    dp.add_handler(CommandHandler(\"aleph\", set_aleph))\n",
        "    dp.add_handler(CommandHandler(\"luminous\",set_aleph))\n",
        "\n",
        "    # on noncommand i.e message - answer on telegram\n",
        "    dp.add_handler(MessageHandler(Filters.TEXT, answer))\n",
        "\n",
        "    # log all errors\n",
        "    dp.add_error_handler(error)\n",
        "\n",
        "    # Start the Bot\n",
        "    updater.start_polling()\n",
        "\n",
        "    # Run the bot until you press Ctrl-C or the process receives SIGINT,\n",
        "    # SIGTERM or SIGABRT. This should be used most of the time, since\n",
        "    # start_polling() is non-blocking and will stop the bot gracefully.\n",
        "    updater.idle()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}